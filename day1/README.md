# Descomplicando o Kubernetes - Expert Mode

## DAY-1

### O quÃª preciso saber antes de comeÃ§ar?

Durante o Day-1 nÃ³s vamos entender o que Ã© um container, vamos falar sobre a importÃ¢ncia do container runtime e do container engine. Durante o Day-1 vamos entender o que Ã© o Kubernetes e sua arquitetura, vamos falar sobre o control plane, workers, apiserver, scheduler, controller e muito mais! SerÃ¡ aqui que iremos criar o nosso primeiro cluster Kubernetes e realizar o deploy de um pod do Nginx. O Day-1 Ã© para que eu possa me sentir mais confortÃ¡vel com o Kubernetes e seus conceitos iniciais.  

### Inicio da aula do Day-1

Qual distro GNU/Linux devo usar?
Devido ao fato de algumas ferramentas importantes, como o systemd e journald, terem se tornado padrÃ£o na maioria das principais distribuiÃ§Ãµes disponÃ­veis hoje, vocÃª nÃ£o deve encontrar problemas para seguir o treinamento, caso vocÃª opte por uma delas, como Ubuntu, Debian, CentOS e afins.  

### Alguns sites que devemos visitar

Abaixo temos os sites oficiais do projeto do Kubernetes:

https://kubernetes.io
https://github.com/kubernetes/kubernetes/
https://github.com/kubernetes/kubernetes/issues

### Abaixo temos as pÃ¡ginas oficiais das certificaÃ§Ãµes do Kubernetes (CKA, CKAD e CKS):

https://www.cncf.io/certification/cka/
https://www.cncf.io/certification/ckad/
https://www.cncf.io/certification/cks/

 

### O Container Engine

Antes de comeÃ§ar a falar um pouco mais sobre o Kubernetes, nÃ³s primeiro precisamos entender alguns componentes que sÃ£o importantes no ecossistema do Kubernetes, um desses componentes Ã© o Container Engine.

O Container Engine Ã© o responsÃ¡vel por gerenciar as imagens e volumes, Ã© ele o responsÃ¡vel por garantir que os os recursos que os containers estÃ£o utilizando estÃ¡ devidamente isolados, a vida do container, storage, rede, etc.

Hoje temos diversas opÃ§Ãµes para se utilizar como Container Engine, que atÃ© pouco tempo atrÃ¡s tinhamos somente o Docker para esse papel.

OpÃ§Ãµes como o Docker, o CRI-O e o Podman sÃ£o bem conhecidas e preparadas para o ambiente produtivo. O Docker, como todos sabem, Ã© o Container Engine mais popular e ele utiliza como Container Runtime o containerd.

### Container Runtime? O que Ã© isso?

Calma que vou te explicar jÃ¡ jÃ¡, mas antes temos que falar sobre a OCI. :)

 

### OCI - Open Container Initiative

A OCI Ã© uma organizaÃ§Ã£o sem fins lucrativos que tem como objetivo padronizar a criaÃ§Ã£o de containers, para que possam ser executados em qualquer ambiente. A OCI foi fundada em 2015 pela Docker, CoreOS, Google, IBM, Microsoft, Red Hat e VMware e hoje faz parte da Linux Foundation.

O principal projeto criado pela OCI Ã© o runc, que Ã© o principal container runtime de baixo nÃ­vel, e utilizado por diferentes *Container Engines, como o Docker. O runc Ã© um projeto open source, escrito em Go e seu cÃ³digo estÃ¡ disponÃ­vel no GitHub.

Agora sim jÃ¡ podemos falar sobre o que Ã© o Container Runtime.
 
#### O Container Runtime

Para que seja possÃ­vel executar os containers nos nÃ³s Ã© necessÃ¡rio ter um Container Runtime instalado em cada um dos nÃ³s.

O Container Runtime Ã© o responsÃ¡vel por executar os containers nos nÃ³s. Quando vocÃª estÃ¡ utilizando Docker ou Podman para executar containers em sua mÃ¡quina, por exemplo, vocÃª estÃ¡ fazendo uso de algum Container Runtime, ou melhor, o seu Container Engine estÃ¡ fazendo uso de algum Container Runtime.

Temos trÃªs tipos de Container Runtime:

Low-level: sÃ£o os Container Runtime que sÃ£o executados diretamente pelo Kernel, como o runc, o crun e o runsc.

High-level: sÃ£o os Container Runtime que sÃ£o executados por um Container Engine, como o containerd, o CRI-O e o Podman.

Sandbox: sÃ£o os Container Runtime que sÃ£o executados por um Container Engine e que sÃ£o responsÃ¡veis por executar containers de forma segura em unikernels ou utilizando algum proxy para fazer a comunicaÃ§Ã£o com o Kernel. O gVisor Ã© um exemplo de Container Runtime do tipo Sandbox.

Virtualized: sÃ£o os Container Runtime que sÃ£o executados por um Container Engine e que sÃ£o responsÃ¡veis por executar containers de forma segura em mÃ¡quinas virtuais. A performance aqui Ã© um pouco menor do que quando temos um sendo executado nativamente. O Kata Containers Ã© um exemplo de Container Runtime do tipo Virtualized.

 

### O que Ã© o Kubernetes?

VersÃ£o resumida:

O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de contÃªineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa "timoneiro", Ã© um projeto open source que conta com design e desenvolvimento baseados no projeto Borg, que tambÃ©m Ã© da Google 1. Alguns outros produtos disponÃ­veis no mercado, tais como o Apache Mesos e o Cloud Foundry, tambÃ©m surgiram a partir do projeto Borg.

Como Kubernetes Ã© uma palavra difÃ­cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de k8s, seguindo o padrÃ£o i18n (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

VersÃ£o longa:

Praticamente todo software desenvolvido na Google Ã© executado em contÃªiner 2. A Google jÃ¡ gerencia contÃªineres em larga escala hÃ¡ mais de uma dÃ©cada, quando nÃ£o se falava tanto sobre isso. Para atender a demanda interna, alguns desenvolvedores do Google construÃ­ram trÃªs sistemas diferentes de gerenciamento de contÃªineres: Borg, Omega e Kubernetes. Cada sistema teve o desenvolvimento bastante influenciado pelo antecessor, embora fosse desenvolvido por diferentes razÃµes.

O primeiro sistema de gerenciamento de contÃªineres desenvolvido no Google foi o Borg, construÃ­do para gerenciar serviÃ§os de longa duraÃ§Ã£o e jobs em lote, que anteriormente eram tratados por dois sistemas: Babysitter e Global Work Queue. O Ãºltimo influenciou fortemente a arquitetura do Borg, mas estava focado em execuÃ§Ã£o de jobs em lote. O Borg continua sendo o principal sistema de gerenciamento de contÃªineres dentro do Google por causa de sua escala, variedade de recursos e robustez extrema.

O segundo sistema foi o Omega, descendente do Borg. Ele foi impulsionado pelo desejo de melhorar a engenharia de software do ecossistema Borg. Esse sistema aplicou muitos dos padrÃµes que tiveram sucesso no Borg, mas foi construÃ­do do zero para ter a arquitetura mais consistente. Muitas das inovaÃ§Ãµes do Omega foram posteriormente incorporadas ao Borg.

O terceiro sistema foi o Kubernetes. Concebido e desenvolvido em um mundo onde desenvolvedores externos estavam se interessando em contÃªineres e o Google desenvolveu um negÃ³cio em amplo crescimento atualmente, que Ã© a venda de infraestrutura de nuvem pÃºblica.

O Kubernetes Ã© de cÃ³digo aberto - em contraste com o Borg e o Omega que foram desenvolvidos como sistemas puramente internos do Google. O Kubernetes foi desenvolvido com um foco mais forte na experiÃªncia de desenvolvedores que escrevem aplicativos que sÃ£o executados em um cluster: seu principal objetivo Ã© facilitar a implantaÃ§Ã£o e o gerenciamento de sistemas distribuÃ­dos, enquanto se beneficia do melhor uso de recursos de memÃ³ria e processamento que os contÃªineres possibilitam.

Estas informaÃ§Ãµes foram extraÃ­das e adaptadas deste artigo, que descreve as liÃ§Ãµes aprendidas com o desenvolvimento e operaÃ§Ã£o desses sistemas.  

### Arquitetura do k8s

Assim como os demais orquestradores disponÃ­veis, o k8s tambÃ©m segue um modelo control plane/workers, constituindo assim um cluster, onde para seu funcionamento Ã© recomendado no mÃ­nimo trÃªs nÃ³s: o nÃ³ control-plane, responsÃ¡vel (por padrÃ£o) pelo gerenciamento do cluster, e os demais como workers, executores das aplicaÃ§Ãµes que queremos executar sobre esse cluster.

Ã‰ possÃ­vel criar um cluster Kubernetes rodando em apenas um nÃ³, porÃ©m Ã© recomendado somente para fins de estudos e nunca executado em ambiente produtivo.

Caso vocÃª queira utilizar o Kubernetes em sua mÃ¡quina local, em seu desktop, existem diversas soluÃ§Ãµes que irÃ£o criar um cluster Kubernetes, utilizando mÃ¡quinas virtuais ou o Docker, por exemplo.

Com isso vocÃª poderÃ¡ ter um cluster Kubernetes com diversos nÃ³s, porÃ©m todos eles rodando em sua mÃ¡quina local, em seu desktop.

Alguns exemplos sÃ£o:

Kind: Uma ferramenta para execuÃ§Ã£o de contÃªineres Docker que simulam o funcionamento de um cluster Kubernetes. Ã‰ utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O Kind nÃ£o deve ser utilizado para produÃ§Ã£o;

Minikube: ferramenta para implementar um cluster Kubernetes localmente com apenas um nÃ³. Muito utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O Minikube nÃ£o deve ser utilizado para produÃ§Ã£o;

MicroK8S: Desenvolvido pela Canonical, mesma empresa que desenvolve o Ubuntu. Pode ser utilizado em diversas distribuiÃ§Ãµes e pode ser utilizado em ambientes de produÃ§Ã£o, em especial para Edge Computing e IoT (Internet of things);

k3s: Desenvolvido pela Rancher Labs, Ã© um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi.

k0s: Desenvolvido pela Mirantis, mesma empresa que adquiriu a parte enterprise do Docker. Ã‰ uma distribuiÃ§Ã£o do Kubernetes com todos os recursos necessÃ¡rios para funcionar em um Ãºnico binÃ¡rio, que proporciona uma simplicidade na instalaÃ§Ã£o e manutenÃ§Ã£o do cluster. A pronÃºncia Ã© correta Ã© kay-zero-ess e tem por objetivo reduzir o esforÃ§o tÃ©cnico e desgaste na instalaÃ§Ã£o de um cluster Kubernetes, por isso o seu nome faz alusÃ£o a Zero Friction. O k0s pode ser utilizado em ambientes de produÃ§Ã£o/

API Server: Ã‰ um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunicaÃ§Ã£o, onde para isto Ã© utilizado principalmente o utilitÃ¡rio kubectl, por parte dos administradores, para a comunicaÃ§Ã£o com os demais nÃ³s, como mostrado no grÃ¡fico. Estas comunicaÃ§Ãµes entre componentes sÃ£o estabelecidas atravÃ©s de requisiÃ§Ãµes REST;

**etcd:** O etcd Ã© um datastore chave-valor distribuÃ­do que o k8s utiliza para armazenar as especificaÃ§Ãµes, status e configuraÃ§Ãµes do cluster. Todos os dados armazenados dentro do etcd sÃ£o manipulados apenas atravÃ©s da API. Por questÃµes de seguranÃ§a, o etcd Ã© por padrÃ£o executado apenas em nÃ³s classificados como control plane no cluster k8s, mas tambÃ©m podem ser executados em clusters externos, especÃ­ficos para o etcd, por exemplo;

**Scheduler:** O scheduler Ã© responsÃ¡vel por selecionar o nÃ³ que irÃ¡ hospedar um determinado pod (a menor unidade de um cluster k8s - nÃ£o se preocupe sobre isso por enquanto, nÃ³s falaremos mais sobre isso mais tarde) para ser executado. Esta seleÃ§Ã£o Ã© feita baseando-se na quantidade de recursos disponÃ­veis em cada nÃ³, como tambÃ©m no estado de cada um dos nÃ³s do cluster, garantindo assim que os recursos sejam bem distribuÃ­dos. AlÃ©m disso, a seleÃ§Ã£o dos nÃ³s, na qual um ou mais pods serÃ£o executados, tambÃ©m pode levar em consideraÃ§Ã£o polÃ­ticas definidas pelo usuÃ¡rio, tais como afinidade, localizaÃ§Ã£o dos dados a serem lidos pelas aplicaÃ§Ãµes, etc;

**Controller Manager:** Ã‰ o controller manager quem garante que o cluster esteja no Ãºltimo estado definido no etcd. Por exemplo: se no etcd um deploy estÃ¡ configurado para possuir dez rÃ©plicas de um pod, Ã© o controller manager quem irÃ¡ verificar se o estado atual do cluster corresponde a este estado e, em caso negativo, procurarÃ¡ conciliar ambos;

**Kubelet:** O kubelet pode ser visto como o agente do k8s que Ã© executado nos nÃ³s workers. Em cada nÃ³ worker deverÃ¡ existir um agente Kubelet em execuÃ§Ã£o. O Kubelet Ã© responsÃ¡vel por de fato gerenciar os pods, que foram direcionados pelo controller do cluster, dentro dos nÃ³s, de forma que para isto o Kubelet pode iniciar, parar e manter os contÃªineres e os pods em funcionamento de acordo com o instruÃ­do pelo controlador do cluster;

**Kube-proxy:** Age como um proxy e um load balancer. Este componente Ã© responsÃ¡vel por efetuar roteamento de requisiÃ§Ãµes para os pods corretos, como tambÃ©m por cuidar da parte de rede do nÃ³;  

Portas que devemos nos preocupar
CONTROL PLANE

## Protocol	Direction	Port Range	Purpose	Used By

> - TCP Inbound 6443* Kubernetes API server	All
> - TCP	Inbound	2379-2380	etcd server client API kube-apiserver, etcd
> - TCP	Inbound	10250	Kubelet API	Self, Control plane
> - TCP	Inbound	10251	kube-scheduler Self
> - TCP	Inbound	10252	kube-controller-manager	Self

Toda porta marcada por * Ã© customizÃ¡vel, vocÃª precisa se certificar que a porta alterada tambÃ©m esteja aberta.   WORKERS
Protocol	Direction	Port Range	Purpose	Used By
>> TCP	Inbound	10250	Kubelet API	Self, Control plane
>> TCP	Inbound	30000-32767	NodePort	Services All
 

Conceitos-chave do k8s
Ã‰ importante saber que a forma como o k8s gerencia os contÃªineres Ã© ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele nÃ£o trata os contÃªineres diretamente, mas sim atravÃ©s de pods. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:

Pod: Ã‰ o menor objeto do k8s. Como dito anteriormente, o k8s nÃ£o trabalha com os contÃªineres diretamente, mas organiza-os dentro de pods, que sÃ£o abstraÃ§Ãµes que dividem os mesmos recursos, como endereÃ§os, volumes, ciclos de CPU e memÃ³ria. Um pod pode possuir vÃ¡rios contÃªineres;

Deployment: Ã‰ um dos principais controllers utilizados. O Deployment, em conjunto com o ReplicaSet, garante que determinado nÃºmero de rÃ©plicas de um pod esteja em execuÃ§Ã£o nos nÃ³s workers do cluster. AlÃ©m disso, o Deployment tambÃ©m Ã© responsÃ¡vel por gerenciar o ciclo de vida das aplicaÃ§Ãµes, onde caracterÃ­sticas associadas a aplicaÃ§Ã£o, tais como imagem, porta, volumes e variÃ¡veis de ambiente, podem ser especificados em arquivos do tipo yaml ou json para posteriormente serem passados como parÃ¢metro para o kubectl executar o deployment. Esta aÃ§Ã£o pode ser executada tanto para criaÃ§Ã£o quanto para atualizaÃ§Ã£o e remoÃ§Ã£o do deployment;

ReplicaSets: Ã‰ um objeto responsÃ¡vel por garantir a quantidade de pods em execuÃ§Ã£o no nÃ³;

Services: Ã‰ uma forma de vocÃª expor a comunicaÃ§Ã£o atravÃ©s de um ClusterIP, NodePort ou LoadBalancer para distribuir as requisiÃ§Ãµes entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.

Instalando e customizando o Kubectl
InstalaÃ§Ã£o do Kubectl no GNU/Linux
Vamos instalar o kubectl com os seguintes comandos.

curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
 

InstalaÃ§Ã£o do Kubectl no MacOS
O kubectl pode ser instalado no MacOS utilizando tanto o Homebrew, quanto o mÃ©todo tradicional. Com o Homebrew jÃ¡ instalado, o kubectl pode ser instalado da seguinte forma.

sudo brew install kubectl

kubectl version --client
  Ou:

sudo brew install kubectl-cli

kubectl version --client
  JÃ¡ com o mÃ©todo tradicional, a instalaÃ§Ã£o pode ser realizada com os seguintes comandos.

curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
 

InstalaÃ§Ã£o do Kubectl no Windows
A instalaÃ§Ã£o do kubectl pode ser realizada efetuando o download neste link.

Outras informaÃ§Ãµes sobre como instalar o kubectl no Windows podem ser encontradas nesta pÃ¡gina.

Customizando o kubectl
Auto-complete
Execute o seguinte comando para configurar o alias e autocomplete para o kubectl.

No Bash:

source <(kubectl completion bash) # configura o autocomplete na sua sessÃ£o atual (antes, certifique-se de ter instalado o pacote bash-completion).

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanentemente ao seu shell.
  No ZSH:

source <(kubectl completion zsh)

echo "[[ $commands[kubectl] ]] && source <(kubectl completion zsh)"
 

Criando um alias para o kubectl
Crie o alias k para kubectl:

alias k=kubectl

complete -F __start_kubectl k
 

Criando um cluster Kubernetes
Criando o cluster em sua mÃ¡quina local
Vamos mostrar algumas opÃ§Ãµes, caso vocÃª queira comeÃ§ar a brincar com o Kubernetes utilizando somente a sua mÃ¡quina local, o seu desktop.

Lembre-se, vocÃª nÃ£o Ã© obrigado a testar/utilizar todas as opÃ§Ãµes abaixo, mas seria muito bom caso vocÃª testasse. :D

Minikube
Requisitos bÃ¡sicos
Ã‰ importante frisar que o Minikube deve ser instalado localmente, e nÃ£o em um cloud provider. Por isso, as especificaÃ§Ãµes de hardware a seguir sÃ£o referentes Ã  mÃ¡quina local.

Processamento: 1 core;
MemÃ³ria: 2 GB;
HD: 20 GB.
InstalaÃ§Ã£o do Minikube no GNU/Linux
Antes de mais nada, verifique se a sua mÃ¡quina suporta virtualizaÃ§Ã£o. No GNU/Linux, isto pode ser realizado com o seguinte comando:

grep -E --color 'vmx|svm' /proc/cpuinfo
  Caso a saÃ­da do comando nÃ£o seja vazia, o resultado Ã© positivo.

HÃ¡ a possibilidade de nÃ£o utilizar um hypervisor para a instalaÃ§Ã£o do Minikube, executando-o ao invÃ©s disso sobre o prÃ³prio host. Iremos utilizar o Oracle VirtualBox como hypervisor, que pode ser encontrado aqui.

Efetue o download e a instalaÃ§Ã£o do Minikube utilizando os seguintes comandos.

> curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
> chmod +x ./minikube
> sudo mv ./minikube /usr/local/bin/minikube

>> minikube version
 

InstalaÃ§Ã£o do Minikube no MacOS
No MacOS, o comando para verificar se o processador suporta virtualizaÃ§Ã£o Ã©:

sysctl -a | grep -E --color 'machdep.cpu.features|VMX'
  Se vocÃª visualizar VMX na saÃ­da, o resultado Ã© positivo.

Efetue a instalaÃ§Ã£o do Minikube com um dos dois mÃ©todos a seguir, podendo optar-se pelo Homebrew ou pelo mÃ©todo tradicional.

sudo brew install minikube

minikube version
  Ou:

curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
 

InstalaÃ§Ã£o do Minikube no Microsoft Windows
No Microsoft Windows, vocÃª deve executar o comando systeminfo no prompt de comando ou no terminal. Caso o retorno deste comando seja semelhante com o descrito a seguir, entÃ£o a virtualizaÃ§Ã£o Ã© suportada.

Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
  Caso a linha a seguir tambÃ©m esteja presente, nÃ£o Ã© necessÃ¡ria a instalaÃ§Ã£o de um hypervisor como o Oracle VirtualBox:

Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
  FaÃ§a o download e a instalaÃ§Ã£o de um hypervisor (preferencialmente o Oracle VirtualBox), caso no passo anterior nÃ£o tenha sido acusada a presenÃ§a de um. Finalmente, efetue o download do instalador do Minikube aqui e execute-o.

Iniciando, parando e excluindo o Minikube
Quando operando em conjunto com um hypervisor, o Minikube cria uma mÃ¡quina virtual, onde dentro dela estarÃ£o todos os componentes do k8s para execuÃ§Ã£o.

Ã‰ possÃ­vel selecionar qual hypervisor iremos utilizar por padrÃ£o, atravÃ©s no comando abaixo:

minikube config set driver <SEU_HYPERVISOR> 
  VocÃª deve substituir <SEU_HYPERVISOR> pelo seu hypervisor, por exemplo o KVM2, QEMU, Virtualbox ou o Hyperkit.

Caso nÃ£o queria configurar um hypervisor padrÃ£o, vocÃª pode digitar o comando minikube start --driver=hyperkit toda vez que criar um novo ambiente.

Certo, e como eu sei que estÃ¡ tudo funcionando como deveria?
Uma vez iniciado, vocÃª deve ter uma saÃ­da na tela similar Ã  seguinte:

minikube start

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Using the qemu2 (experimental) driver based on user configuration
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸ”¥  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

VocÃª pode entÃ£o listar os nÃ³s que fazem parte do seu cluster k8s com o seguinte comando:

kubectl get nodes
  A saÃ­da serÃ¡ similar ao conteÃºdo a seguir:

kubectl get nodes
  Para criar um cluster com mais de um nÃ³, vocÃª pode utilizar o comando abaixo, apenas modificando os valores para o desejado:

minikube start --nodes 2 -p multinode-cluster

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ’¾  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

ğŸ‘  Starting worker node minikube-m02 in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸŒ  Found network options:
    â–ª NO_PROXY=192.168.11.11
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª env NO_PROXY=192.168.11.11
ğŸ”  Verifying Kubernetes components...
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

  Para visualizar os nÃ³s do seu novo cluster Kubernetes, digite:

kubectl get nodes
  Inicialmente, a intenÃ§Ã£o do Minikube Ã© executar o k8s em apenas um nÃ³, porÃ©m a partir da versÃ£o 1.10.1 e possÃ­vel usar a funÃ§Ã£o de multi-node.

Caso os comandos anteriores tenham sido executados sem erro, a instalaÃ§Ã£o do Minikube terÃ¡ sido realizada com sucesso.

Ver detalhes sobre o cluster
minikube status
 

Descobrindo o endereÃ§o do Minikube
Como dito anteriormente, o Minikube irÃ¡ criar uma mÃ¡quina virtual, assim como o ambiente para a execuÃ§Ã£o do k8s localmente. Ele tambÃ©m irÃ¡ configurar o kubectl para comunicar-se com o Minikube. Para saber qual Ã© o endereÃ§o IP dessa mÃ¡quina virtual, pode-se executar:

minikube ip
  O endereÃ§o apresentado Ã© que deve ser utilizado para comunicaÃ§Ã£o com o k8s.

Acessando a mÃ¡quina do Minikube via SSH
Para acessar a mÃ¡quina virtual criada pelo Minikube, pode-se executar:

minikube ssh
 

Dashboard do Minikube
O Minikube vem com um dashboard web interessante para que o usuÃ¡rio iniciante observe como funcionam os workloads sobre o k8s. Para habilitÃ¡-lo, o usuÃ¡rio pode digitar:

minikube dashboard
 

Logs do Minikube
Os logs do Minikube podem ser acessados atravÃ©s do seguinte comando.

minikube logs
 

Remover o cluster
minikube delete
  Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro --purge, conforme abaixo:

minikube delete --purge
 

Kind
O Kind (Kubernetes in Docker) Ã© outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas nÃ£o Ã© recomendado para uso em produÃ§Ã£o.

InstalaÃ§Ã£o no GNU/Linux
Para fazer a instalaÃ§Ã£o no GNU/Linux, execute os seguintes comandos.

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
 

InstalaÃ§Ã£o no MacOS
Para fazer a instalaÃ§Ã£o no MacOS, execute o seguinte comando.

sudo brew install kind
  ou

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-darwin-amd64
chmod +x ./kind
mv ./kind /usr/bin/kind
 

InstalaÃ§Ã£o no Windows
Para fazer a instalaÃ§Ã£o no Windows, execute os seguintes comandos.

curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
 

InstalaÃ§Ã£o no Windows via Chocolatey
Execute o seguinte comando para instalar o Kind no Windows usando o Chocolatey.

choco install kind
 

Criando um cluster com o Kind
ApÃ³s realizar a instalaÃ§Ã£o do Kind, vamos iniciar o nosso cluster.

kind create cluster

Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

  Ã‰ possÃ­vel criar mais de um cluster e personalizar o seu nome.

kind create cluster --name giropops

Creating cluster "giropops" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-giropops"
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! ğŸ˜Š
  Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.

kind get clusters
  Liste os nodes do cluster.

kubectl get nodes
 

Criando um cluster com mÃºltiplos nÃ³s locais com o Kind
Ã‰ possÃ­vel para essa aula incluir mÃºltiplos nÃ³s na estrutura do Kind, que foi mencionado anteriormente.

Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.

kind delete clusters $(kind get clusters)

Deleted clusters: ["giropops" "kind"]
  Crie um arquivo de configuraÃ§Ã£o para definir quantos e o tipo de nÃ³s no cluster que vocÃª deseja. No exemplo a seguir, serÃ¡ criado o arquivo de configuraÃ§Ã£o kind-3nodes.yaml para especificar um cluster com 1 nÃ³ control-plane (que executarÃ¡ o control plane) e 2 workers.

cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
  Agora vamos criar um cluster chamado kind-multinodes utilizando as especificaÃ§Ãµes definidas no arquivo kind-3nodes.yaml.

kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster "kind-multinodes" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
 âœ“ Joining worker nodes ğŸšœ 
Set kubectl context to "kind-kind-multinodes"
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ğŸ™‚
  Valide a criaÃ§Ã£o do cluster com o comando a seguir.

kubectl get nodes
  Mais informaÃ§Ãµes sobre o Kind estÃ£o disponÃ­veis em: https://kind.sigs.k8s.io

 

Primeiros passos no k8s
 

Verificando os namespaces e pods
O k8s organiza tudo dentro de namespaces. Por meio deles, podem ser realizadas limitaÃ§Ãµes de seguranÃ§a e de recursos dentro do cluster, tais como pods, replication controllers e diversos outros. Para visualizar os namespaces disponÃ­veis no cluster, digite:

kubectl get namespaces
  Vamos listar os pods do namespace kube-system utilizando o comando a seguir.

kubectl get pod -n kube-system
  SerÃ¡ que hÃ¡ algum pod escondido em algum namespace? Ã‰ possÃ­vel listar todos os pods de todos os namespaces com o comando a seguir.

kubectl get pods -A
  HÃ¡ a possibilidade ainda, de utilizar o comando com a opÃ§Ã£o -o wide, que disponibiliza maiores informaÃ§Ãµes sobre o recurso, inclusive em qual nÃ³ o pod estÃ¡ sendo executado. Exemplo:

kubectl get pods -A -o wide
 

Executando nosso primeiro pod no k8s
Iremos iniciar o nosso primeiro pod no k8s. Para isso, executaremos o comando a seguir.

kubectl run nginx --image nginx

pod/nginx created
  Listando os pods com kubectl get pods, obteremos a seguinte saÃ­da.

NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
  Vamos agora remover o nosso pod com o seguinte comando.

kubectl delete pod nginx
  A saÃ­da deve ser algo como:

pod "nginx" deleted
 

Executando nosso primeiro pod no k8s
Uma outra forma de criar um pod ou qualquer outro objeto no Kubernetes Ã© atravÃ©s da utilizaÃ§Ã¢o de uma arquivo manifesto, que Ã© uma arquivo em formato YAML onde vocÃª passa todas as definiÃ§Ãµes do seu objeto. Mas pra frente vamos falar muito mais sobre como construir arquivos manifesto, mas agora eu quero que vocÃª conheÃ§a a opÃ§Ã£o --dry-run do kubectl, pos com ele podemos simular a criaÃ§Ã£o de um resource e ainda ter um manifesto criado automaticamente.

Exemplos:

Para a criaÃ§Ã£o do template de um pod:

kubectl run meu-nginx --image nginx --dry-run=client -o yaml > pod-template.yaml
  Aqui estamos utilizando ainda o parametro '-o', utilizando para modificar a saÃ­da para o formato YAML.

Para a criaÃ§Ã£o do template de um deployment:

Com o arquivo gerado em mÃ£os, agora vocÃª consegue criar um pod utilizando o manifesto que criamos da seguinte forma:

kubectl apply -f pod-template.yaml
NÃ£o se preocupe por enquanto com o parametro 'apply', nÃ³s ainda vamos falar com mais detalhes sobre ele, nesse momento o importante Ã© vocÃª saber que ele Ã© utilizado para criar novos resources atravÃ©s de arquivos manifestos.

 

Expondo o pod e criando um Service
Dispositivos fora do cluster, por padrÃ£o, nÃ£o conseguem acessar os pods criados, como Ã© comum em outros sistemas de contÃªineres. Para expor um pod, execute o comando a seguir.

kubectl expose pod nginx
SerÃ¡ apresentada a seguinte mensagem de erro:

error: couldn't find port via --port flag or introspection
See 'kubectl expose -h' for help and examples
O erro ocorre devido ao fato do k8s nÃ£o saber qual Ã© a porta de destino do contÃªiner que deve ser exposta (no caso, a 80/TCP). Para configurÃ¡-la, vamos primeiramente remover o nosso pod antigo:

kubectl delete -f pod-template.yaml
Agora vamos executar novamente o comando para a criaÃ§Ã£o do pod utilizando o parametro 'dry-run', porÃ©m agora vamos adicionar o parametro '--port' para dizer qual a porta que o container estÃ¡ escutando, lembrando que estamos utilizando o nginx nesse exemplo, um webserver que escuta por padrÃ£o na porta 80.

kubectl run meu-nginx --image nginx --dry-run=client -o yaml > pod-template.yaml
kubectl create -f pod-template.yaml
Liste os pods.

kubectl get pods

NAME    READY   STATUS    RESTARTS   AGE
meu-nginx   1/1     Running   0          32s
O comando a seguir cria um objeto do k8s chamado de Service, que Ã© utilizado justamente para expor pods para acesso externo.

kubectl expose pod meu-nginx
Podemos listar todos os services com o comando a seguir.

kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8d
nginx        ClusterIP   10.105.41.192   <none>        80/TCP    2m30s
Como Ã© possÃ­vel observar, hÃ¡ dois services no nosso cluster: o primeiro Ã© para uso do prÃ³prio k8s, enquanto o segundo foi o quÃª acabamos de criar.

 

Limpando tudo e indo para casa
Para mostrar todos os recursos recÃ©m criados, pode-se utilizar uma das seguintes opÃ§Ãµes a seguir.

kubectl get all

kubectl get pod,service

kubectl get pod,svc
Note que o k8s nos disponibiliza algumas abreviaÃ§Ãµes de seus recursos. Com o tempo vocÃª irÃ¡ se familiar com elas. Para apagar os recursos criados, vocÃª pode executar os seguintes comandos.

kubectl delete -f pod-template.yaml
kubectl delete service nginx
Liste novamente os recursos para ver se os mesmos ainda estÃ£o presentes.

